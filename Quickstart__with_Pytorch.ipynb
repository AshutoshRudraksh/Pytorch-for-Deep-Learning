{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQV2vMyC7pWDBuWjIJZGNu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stac-bot/Pytorch-for-Deep-Learning/blob/main/Quickstart__with_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Working with data"
      ],
      "metadata": {
        "id": "d1lcybIODNrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the core of PyTorch data loading utility is torch.utils.data.DataLoader class.<br> What is in it for us? good question? <br>\n",
        "**Data Loader combines datasets and sampler, and provides and iterable over the given datasets it also works for both map-style and iterable-style datasets**\n",
        "It has python iterable over the dataset, that support various operations.<br>\n",
        "1. map-style (protocols:(__getitem__(),__len__()) and iterable-style datasets(__iter__()),\n",
        "\n",
        "2. customizing data loading order(user defined Sampler- works only with map-style, since iterable-style datasets have no notion of key or an index),\n",
        "\n",
        "3. automatic batching (take a look at collate_fn https://pytorch.org/docs/stable/data.html#working-with-collate-fn),\n",
        "\n",
        "4. single- and multi-process data loading (https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading),<br> `Note:number of workers * size of parent process`\n",
        "\n",
        "5. automatic memory pinning (for faster data transfer- https://pytorch.org/docs/stable/data.html#memory-pinning ). \n",
        " \n",
        " DatLoader is an constructor which has all of this options\n"
      ],
      "metadata": {
        "id": "FIKgg319DX5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sub-task 1: import domain-specific libraries"
      ],
      "metadata": {
        "id": "H69EDIWlNBAH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yGjyEnqEC7_i"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.cs.toronto.edu/~kriz/cifar.html - dive deep into dataset"
      ],
      "metadata": {
        "id": "RixFmRwLOD8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download training data from open datasets\n",
        "training_data = datasets.FashionMNIST(root=\"data\", # path of training data\n",
        "                                      train = True, # specifies training set or test datasets\n",
        "                                      download = True, # download the data from internet if not avail locally \n",
        "                                      transform = ToTensor(), # specify the feature label transformations\n",
        "                                      ) \n",
        "\n",
        "# download test data from open datasets \n",
        "test_data =  datasets.FashionMNIST(root=\"data\",\n",
        "                                   train=\"False\",\n",
        "                                   download = True,\n",
        "                                   transform = ToTensor())"
      ],
      "metadata": {
        "id": "4XY5aJOrOH2j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sub-task 2: now we pass our dataset to an argument DataLoader, as we recall which provides iterable over *dataset, and support automatic batching, sampling, shuffling, and multiprocess data loading*"
      ],
      "metadata": {
        "id": "g1plE6STPdjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size = batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "  print(f\"Shape of X[N, C, H, W]: {X.shape}\")\n",
        "  print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxNAzmRXPIj9",
        "outputId": "eda5a747-db01-48fc-d140-25fdf10ff947"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X[N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 2 : creating Model"
      ],
      "metadata": {
        "id": "ry4W2BWVSz94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to define neural network we always create a class that inherits from nn.Module and then we define layers of networks in the __init__ function further defining how data will pass through the network n Forword function. <br>\n",
        "for boost our work we move it to GPU if avail "
      ],
      "metadata": {
        "id": "JbqvJu3NS7YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu or gpu device for training\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# defince our model\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,10)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "# get the model on same device\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9lrA8g7RNvr",
        "outputId": "8819ee2f-c531-4d39-ba63-2becd8ac1fc1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: train the model "
      ],
      "metadata": {
        "id": "858AiOlEWj14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for training the model we need loss function an optimizer"
      ],
      "metadata": {
        "id": "qt2VGFiMWqVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "3XLRU7ehViTh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the training loop\n",
        "def train(dataloader, model,loss_fn, optimizer):\n",
        "  size = len(dataloader)\n",
        "  model.train()\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # Compute prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred,y)\n",
        "\n",
        "    #backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "# define the test loop\n",
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss , correct = 0,0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred,y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")"
      ],
      "metadata": {
        "id": "wPTgQVTIXdUA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sub-tak : training process is conducted over several iterations(epochs). What happens here?<br>\n",
        "Model learns parameters to make better predictions."
      ],
      "metadata": {
        "id": "EJvIEiQCaKwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2 \n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n---------------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "  test(test_dataloader,model,loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baqbK-j2ZFo9",
        "outputId": "d114d7f1-e565-4624-f1f7-1e583082d707"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "---------------------------------\n",
            "loss: 2.304174 [    0/  938]\n",
            "loss: 2.145146 [ 6400/  938]\n",
            "loss: 1.759648 [12800/  938]\n",
            "loss: 1.497524 [19200/  938]\n",
            "loss: 1.140288 [25600/  938]\n",
            "loss: 1.036516 [32000/  938]\n",
            "loss: 1.005391 [38400/  938]\n",
            "loss: 0.861935 [44800/  938]\n",
            "loss: 0.865637 [51200/  938]\n",
            "loss: 0.803440 [57600/  938]\n",
            "Test Error: \n",
            " Accuracy: 72.6%, Avg loss: 0.771530\n",
            "\n",
            "Epoch 2\n",
            "---------------------------------\n",
            "loss: 0.788761 [    0/  938]\n",
            "loss: 0.844542 [ 6400/  938]\n",
            "loss: 0.574272 [12800/  938]\n",
            "loss: 0.780884 [19200/  938]\n",
            "loss: 0.673564 [25600/  938]\n",
            "loss: 0.647559 [32000/  938]\n",
            "loss: 0.720643 [38400/  938]\n",
            "loss: 0.682259 [44800/  938]\n",
            "loss: 0.706926 [51200/  938]\n",
            "loss: 0.640689 [57600/  938]\n",
            "Test Error: \n",
            " Accuracy: 78.9%, Avg loss: 0.612661\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model "
      ],
      "metadata": {
        "id": "DfvEahn9ecqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),\"model.path\")\n",
        "print(\"saved PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqU6EzIMZHoB",
        "outputId": "0ba9770b-7ea2-454f-cc31-3915500664cb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved PyTorch Model State to model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AsMiKHIHeq6k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}